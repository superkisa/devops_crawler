{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import os\n",
    "import httpx\n",
    "import asyncio\n",
    "import aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sled.fandom.com/ru/wiki/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FANDOM = \"sled\"\n",
    "WIKI_URL = f\"https://{FANDOM}.fandom.com\"\n",
    "URL_TAIL = f\"/ru/wiki/\"\n",
    "URL = f\"{WIKI_URL}{URL_TAIL}\"\n",
    "URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {\n",
    "    \"User-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "resp = requests.get(URL, headers=headers)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_page_links(page_tag: bs4.Tag) -> list[str]:\n",
    "    links_soup = body.find_all(\"a\", href=re.compile(f\"{URL_TAIL}\"), class_=False)  # type: ignore\n",
    "    return [WIKI_URL + link.get(\"href\") for link in links_soup]\n",
    "\n",
    "\n",
    "def _get_img_tags(page_tag: bs4.Tag) -> bs4.ResultSet[bs4.Tag]:\n",
    "    imgs_soup = body.find_all(\"img\", src=re.compile(\"static.wikia.nocookie.net\"))  # type: ignore\n",
    "    # return [img.get(\"src\") for img in imgs_soup]\n",
    "    return imgs_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(url):\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    soup = bs4.BeautifulSoup(resp.content, 'lxml', from_encoding=\"utf8\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _download_image(img_tag: bs4.Tag, file_path: str):\n",
    "    alt_name = img_tag.get(\"alt\")\n",
    "    file_name = img_tag.get(\"data-image-name\")\n",
    "    img_tag.get(\"src\")\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(img_tag.get(\"src\"))  # type: ignore\n",
    "        if response.status_code == 200:\n",
    "            async with aiofiles.open(f\"{file_path}/{alt_name}_{file_name}\", \"wb\") as f:\n",
    "                await f.write(response.content)\n",
    "            print(f\"Downloaded {file_name}\")\n",
    "\n",
    "\n",
    "async def download_images(img_soup: bs4.ResultSet[bs4.Tag], file_path: str):\n",
    "    tasks = []\n",
    "    for tag in img_soup:\n",
    "        task = asyncio.create_task(_download_image(tag, file_path))\n",
    "        tasks.append(task)\n",
    "    # Wait for all the tasks to complete\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parse_url(url: str):\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = bs4.BeautifulSoup(response.content, 'lxml', from_encoding=\"utf8\")\n",
    "            return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_links = []\n",
    "page_soup = await parse_url(\"https://sled.fandom.com/ru/wiki/%D0%9E%D0%BB%D1%8C%D0%B3%D0%B0_%D0%94%D1%83%D0%BD%D0%B0%D0%B5%D0%B2%D0%B0\")\n",
    "pagetag = page_soup.select_one(\"#mw-content-text\")\n",
    "try:\n",
    "    title = page_soup.title.text.rstrip(\"| След вики | Fandom\")\n",
    "except:\n",
    "    title = page_soup.title.text\n",
    "savedir = f\"data/{title}\"\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "text = pagetag.get_text()\n",
    "with open(f\"{savedir}/{title}.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(text) # save text to txt file\n",
    "page_links.extend(_get_page_links(pagetag))\n",
    "os.makedirs(f\"{savedir}/imgs\", exist_ok=True)\n",
    "await download_images(_get_img_tags(pagetag), f\"{savedir}/imgs\") # save imgs to files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
